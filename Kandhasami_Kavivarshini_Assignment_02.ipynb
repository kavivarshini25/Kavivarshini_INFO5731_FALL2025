{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavivarshini25/Kavivarshini_INFO5731_FALL2025/blob/main/Kandhasami_Kavivarshini_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cca74752-d8f4-4113-bcd1-f511f0eb85ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All narrs' data saved to 'List_of_narrs_collected.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d145642c-680d-4681-ae74-edd0fa7da756\", \"List_of_narrs_collected.csv\", 626746)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import csv\n",
        "from google.colab import files\n",
        "page_url = \"https://ddr.densho.org/narrators/?page={}\"\n",
        "\n",
        "def extract_narr_info():\n",
        "    narrs_all = []\n",
        "\n",
        "    for page_num in range(1, 42):\n",
        "        wblink_1 = Request(page_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        wburl_1 = urlopen(wblink_1)\n",
        "        wbdata_1 = wburl_1.read()\n",
        "        wbdata_1_soup = BeautifulSoup(wbdata_1, 'html.parser')\n",
        "\n",
        "        for narr_link in wbdata_1_soup.find_all('h4'):\n",
        "            wblink_2 = Request(narr_link.a.get('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            wburl_2 = urlopen(wblink_2)\n",
        "            wbdata_2 = wburl_2.read()\n",
        "            wbdata_2_soup = BeautifulSoup(wbdata_2, 'html.parser')\n",
        "\n",
        "            narr = wbdata_2_soup.find(\"div\", attrs={'class': 'col-sm-8 col-md-8'})\n",
        "            narr_name = narr.h1.text.strip().replace('\"', \"\")\n",
        "            narr_bio = narr.p.text.strip() if narr.p else \"\"\n",
        "\n",
        "            interview_titles = []\n",
        "            dates_locations = []\n",
        "            densho_ids = []\n",
        "\n",
        "            for interview in wbdata_2_soup.find_all(\"div\", attrs={'class': 'media'}):\n",
        "                interview_title = interview.find(\"b\", attrs={'class': 'media-heading'}).text.strip()\n",
        "                interview_details = interview.find(\"div\", attrs={'class': 'source muted'})\n",
        "                date_location = \"\"\n",
        "\n",
        "                if interview_details:\n",
        "                    details_text = interview_details.get_text(\"\\n\").strip().split(\"\\n\")\n",
        "                    if len(details_text) >= 2:\n",
        "                        date = details_text[0].strip()\n",
        "                        location = details_text[1].strip()\n",
        "                        date_location = f\"{date}, {location}\"\n",
        "\n",
        "                interview_link = interview.find(\"a\", href=True)\n",
        "                densho_id = interview_link['href'].split('/')[-2] if interview_link else \"\"\n",
        "\n",
        "                interview_titles.append(interview_title)\n",
        "                dates_locations.append(date_location)\n",
        "                densho_ids.append(densho_id)\n",
        "\n",
        "            narrs_all.append({\n",
        "                'narr Name': narr_name,\n",
        "                'Bio': narr_bio,\n",
        "                'Interview Title': \", \".join(interview_titles),\n",
        "                'Date & Location': \", \".join(dates_locations),\n",
        "                'Densho ID': \", \".join(densho_ids)\n",
        "            })\n",
        "\n",
        "    csv_file = \"List_of_narrs_collected.csv\"\n",
        "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\n",
        "            'narr Name', 'Bio', 'Interview Title', 'Date & Location', 'Densho ID'\n",
        "        ])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(narrs_all)\n",
        "\n",
        "    print(f\"All narrs' data saved to '{csv_file}'.\")\n",
        "    files.download(csv_file)\n",
        "\n",
        "extract_narr_info()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from google.colab import files\n",
        "\n",
        "# Download NLTK resources (only once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load your CSV\n",
        "my_file = pd.read_csv(\"List_of_narrs_collected.csv\")\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stmr = PorterStemmer()\n",
        "lemtzr = WordNetLemmatizer()\n",
        "stp_wrds = set(stopwords.words('english'))\n",
        "\n",
        "def final_clean_txt(text, show_steps=False):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    original_text = text\n",
        "\n",
        "    # 1. Remove noise: special characters and punctuations\n",
        "    text_no_noise = re.sub(r\"[^\\w\\s]\", \" \", original_text)\n",
        "\n",
        "    # 2. Remove numbers\n",
        "    text_no_numbers = re.sub(r\"\\d+\", \"\", text_no_noise)\n",
        "\n",
        "    # 3. Lowercase all text\n",
        "    text_lower = text_no_numbers.lower()\n",
        "\n",
        "    # 4. Tokenize and remove stopwords\n",
        "    words = text_lower.split()\n",
        "    words_no_stop = [word for word in words if word not in stp_wrds]\n",
        "\n",
        "    # 5. Stemming\n",
        "    words_stemmed = [stmr.stem(word) for word in words_no_stop]\n",
        "\n",
        "    # 6. Lemmatization\n",
        "    words_lemmatized = [lemtzr.lemmatize(word) for word in words_stemmed]\n",
        "\n",
        "    final_cleaned_txt = \" \".join(words_lemmatized)\n",
        "\n",
        "    # Show intermediate results for one sample\n",
        "    if show_steps:\n",
        "        print(\"Original Text:\\n\", original_text, \"\\n\")\n",
        "        print(\"Step 1 - No Noise:\\n\", text_no_noise, \"\\n\")\n",
        "        print(\"Step 2 - No Numbers:\\n\", text_no_numbers, \"\\n\")\n",
        "        print(\"Step 3 - Lowercase:\\n\", text_lower, \"\\n\")\n",
        "        print(\"Step 4 - Stopwords Removed:\\n\", \" \".join(words_no_stop), \"\\n\")\n",
        "        print(\"Step 5 - Stemming:\\n\", \" \".join(words_stemmed), \"\\n\")\n",
        "        print(\"Step 6 - Lemmatization (Final Cleaned):\\n\", final_cleaned_txt, \"\\n\")\n",
        "\n",
        "    return final_cleaned_txt\n",
        "\n",
        "# Apply cleaning to the Bio column\n",
        "my_file['Cleaned Bio'] = my_file['Bio'].apply(final_clean_txt)\n",
        "\n",
        "# Show cleaning steps for the first Bio\n",
        "print(\"=== Cleaning Steps for First Bio ===\")\n",
        "_ = final_clean_txt(my_file['Bio'].iloc[0], show_steps=True)\n",
        "\n",
        "# Show first 5 bios (original vs cleaned)\n",
        "print(\"\\n=== Preview of First 5 Bios (Original vs Cleaned) ===\")\n",
        "print(my_file[['Bio', 'Cleaned Bio']].head(5))\n",
        "\n",
        "# Save to new CSV\n",
        "fout_fl = \"Final_List_of_narrs_cleaned.csv\"\n",
        "my_file.to_csv(fout_fl, index=False)\n",
        "print(\"\\n Cleaned data is saved to 'Final_List_of_narrs_cleaned.csv'.\")\n",
        "\n",
        "# Download file directly in Colab\n",
        "files.download(fout_fl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "bCnihABLhowI",
        "outputId": "0dced015-d68d-4b47-b82d-9209683424d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Cleaning Steps for First Bio ===\n",
            "Original Text:\n",
            " Nisei female. Born May 9, 1927, in Selleck, Washington. Spent much of childhood in Beaverton, Oregon, where father owned a farm. Influenced at an early age by parents' conversion to Christianity. During World War II, removed to the Portland Assembly Center, Oregon, and the Minidoka concentration camp, Idaho. After the war, worked to establish a successful volunteer program to feed the homeless in Seattle, Washington. \n",
            "\n",
            "Step 1 - No Noise:\n",
            " Nisei female  Born May 9  1927  in Selleck  Washington  Spent much of childhood in Beaverton  Oregon  where father owned a farm  Influenced at an early age by parents  conversion to Christianity  During World War II  removed to the Portland Assembly Center  Oregon  and the Minidoka concentration camp  Idaho  After the war  worked to establish a successful volunteer program to feed the homeless in Seattle  Washington  \n",
            "\n",
            "Step 2 - No Numbers:\n",
            " Nisei female  Born May     in Selleck  Washington  Spent much of childhood in Beaverton  Oregon  where father owned a farm  Influenced at an early age by parents  conversion to Christianity  During World War II  removed to the Portland Assembly Center  Oregon  and the Minidoka concentration camp  Idaho  After the war  worked to establish a successful volunteer program to feed the homeless in Seattle  Washington  \n",
            "\n",
            "Step 3 - Lowercase:\n",
            " nisei female  born may     in selleck  washington  spent much of childhood in beaverton  oregon  where father owned a farm  influenced at an early age by parents  conversion to christianity  during world war ii  removed to the portland assembly center  oregon  and the minidoka concentration camp  idaho  after the war  worked to establish a successful volunteer program to feed the homeless in seattle  washington  \n",
            "\n",
            "Step 4 - Stopwords Removed:\n",
            " nisei female born may selleck washington spent much childhood beaverton oregon father owned farm influenced early age parents conversion christianity world war ii removed portland assembly center oregon minidoka concentration camp idaho war worked establish successful volunteer program feed homeless seattle washington \n",
            "\n",
            "Step 5 - Stemming:\n",
            " nisei femal born may selleck washington spent much childhood beaverton oregon father own farm influenc earli age parent convers christian world war ii remov portland assembl center oregon minidoka concentr camp idaho war work establish success volunt program feed homeless seattl washington \n",
            "\n",
            "Step 6 - Lemmatization (Final Cleaned):\n",
            " nisei femal born may selleck washington spent much childhood beaverton oregon father own farm influenc earli age parent convers christian world war ii remov portland assembl center oregon minidoka concentr camp idaho war work establish success volunt program feed homeless seattl washington \n",
            "\n",
            "\n",
            "=== Preview of First 5 Bios (Original vs Cleaned) ===\n",
            "                                                 Bio  \\\n",
            "0  Nisei female. Born May 9, 1927, in Selleck, Wa...   \n",
            "1  Nisei male. Born June 12, 1921, in Seattle, Wa...   \n",
            "2  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "3  Nisei female. Born July 8, 1928, in Boyle Heig...   \n",
            "4  Sansei male. Born March 15, 1950, in Torrance,...   \n",
            "\n",
            "                                         Cleaned Bio  \n",
            "0  nisei femal born may selleck washington spent ...  \n",
            "1  nisei male born june seattl washington grew ar...  \n",
            "2  nisei femal born octob seattl washington famil...  \n",
            "3  nisei femal born juli boyl height california e...  \n",
            "4  sansei male born march torranc california grew...  \n",
            "\n",
            " Cleaned data is saved to 'Final_List_of_narrs_cleaned.csv'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_659c2183-95f6-481d-84b4-f4b157444ce8\", \"Final_List_of_narrs_cleaned.csv\", 936648)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa1039e-0261-4500-99e0-a07714e50cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "POS Analysis Completed!\n",
            "Individual counts saved to pst_anal_results.csv\n",
            "Total counts saved to Total_po_cnts.csv\n",
            "\n",
            "=== Dependency Parsing for First Narrator ===\n",
            "nisei           -> compound   (head: femal)\n",
            "femal           -> nsubj      (head: spent)\n",
            "born            -> acl        (head: femal)\n",
            "may             -> aux        (head: spent)\n",
            "selleck         -> compound   (head: washington)\n",
            "washington      -> nsubj      (head: spent)\n",
            "spent           -> ROOT       (head: spent)\n",
            "much            -> amod       (head: beaverton)\n",
            "childhood       -> compound   (head: beaverton)\n",
            "beaverton       -> compound   (head: father)\n",
            "oregon          -> compound   (head: father)\n",
            "father          -> nmod       (head: center)\n",
            "own             -> amod       (head: convers)\n",
            "farm            -> compound   (head: influenc)\n",
            "influenc        -> nmod       (head: convers)\n",
            "earli           -> amod       (head: convers)\n",
            "age             -> compound   (head: parent)\n",
            "parent          -> compound   (head: convers)\n",
            "convers         -> compound   (head: center)\n",
            "christian       -> compound   (head: center)\n",
            "world           -> compound   (head: center)\n",
            "war             -> compound   (head: ii)\n",
            "ii              -> nmod       (head: center)\n",
            "remov           -> compound   (head: portland)\n",
            "portland        -> compound   (head: center)\n",
            "assembl         -> compound   (head: center)\n",
            "center          -> compound   (head: minidoka)\n",
            "oregon          -> compound   (head: minidoka)\n",
            "minidoka        -> compound   (head: concentr)\n",
            "concentr        -> compound   (head: work)\n",
            "camp            -> compound   (head: work)\n",
            "idaho           -> compound   (head: war)\n",
            "war             -> compound   (head: work)\n",
            "work            -> dobj       (head: spent)\n",
            "establish       -> advcl      (head: spent)\n",
            "success         -> compound   (head: program)\n",
            "volunt          -> compound   (head: program)\n",
            "program         -> compound   (head: feed)\n",
            "feed            -> compound   (head: washington)\n",
            "homeless        -> compound   (head: seattl)\n",
            "seattl          -> compound   (head: washington)\n",
            "washington      -> npadvmod   (head: spent)\n",
            "\n",
            "=== Named Entity Recognition Counts ===\n",
            "PERSON: 1751\n",
            "ORG: 662\n",
            "GPE: 346\n",
            "DATE: 833\n",
            "CARDINAL: 73\n",
            "FAC: 39\n",
            "LOC: 26\n",
            "ORDINAL: 8\n",
            "PRODUCT: 7\n",
            "NORP: 52\n",
            "MONEY: 2\n",
            "TIME: 2\n",
            "EVENT: 13\n",
            "WORK_OF_ART: 5\n",
            "PERCENT: 1\n",
            "LAW: 4\n",
            "QUANTITY: 4\n",
            "LANGUAGE: 2\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "# 1. Install and import required libraries\n",
        "!pip install spacy\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# 2. Load spaCy English model\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 3. Load cleaned CSV file\n",
        "my_file = \"Final_List_of_narrs_cleaned.csv\"\n",
        "fp = pd.read_csv(my_file, dtype=str).fillna('')\n",
        "\n",
        "# 4. ===============================\n",
        "# PART 1: POS TAGGING\n",
        "# ===============================\n",
        "def pst_anal(text):\n",
        "    \"\"\"Count Nouns, Verbs, Adjectives, Adverbs in a text\"\"\"\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return {'N': 0, 'V': 0, 'Adj': 0, 'Adv': 0}\n",
        "\n",
        "    recrd = nlp(text)\n",
        "    po_cnts = {'N': 0, 'V': 0, 'Adj': 0, 'Adv': 0}\n",
        "    for token in recrd:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            po_cnts['N'] += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            po_cnts['V'] += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            po_cnts['Adj'] += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            po_cnts['Adv'] += 1\n",
        "    return po_cnts\n",
        "\n",
        "# Apply POS analysis to the 'Cleaned Bio' column\n",
        "fp['po_cnts'] = fp['Cleaned Bio'].apply(pst_anal)\n",
        "pos_fp = pd.DataFrame(fp['po_cnts'].tolist())\n",
        "\n",
        "# Save individual POS counts\n",
        "pos_fp.to_csv(\"/content/pst_anal_results.csv\", index=False)\n",
        "\n",
        "# Save total counts\n",
        "tt_cnts = pd.DataFrame([pos_fp.sum()])\n",
        "tt_cnts.to_csv(\"/content/Total_po_cnts.csv\", index=False)\n",
        "\n",
        "print(\"POS Analysis Completed!\")\n",
        "print(\"Individual counts saved to pst_anal_results.csv\")\n",
        "print(\"Total counts saved to Total_po_cnts.csv\\n\")\n",
        "\n",
        "# 5. ===============================\n",
        "# PART 2: Dependency Parsing\n",
        "# ===============================\n",
        "def parse_dependencies(text):\n",
        "    \"\"\"Return dependency parse: (word, relation, head)\"\"\"\n",
        "    recrd = nlp(text)\n",
        "    return [(token.text, token.dep_, token.head.text) for token in recrd]\n",
        "\n",
        "# Use first narrator as example\n",
        "if \"Cleaned Bio\" in fp.columns and not fp.empty:\n",
        "    example_text = fp.loc[0, 'Cleaned Bio']\n",
        "    dep_tree = parse_dependencies(example_text)\n",
        "\n",
        "    print(\"=== Dependency Parsing for First Narrator ===\")\n",
        "    for word, relation, head in dep_tree:\n",
        "        print(f\"{word:15} -> {relation:10} (head: {head})\")\n",
        "else:\n",
        "    print(\"No valid 'Cleaned Bio' column found.\")\n",
        "\n",
        "# 6. ===============================\n",
        "# PART 3: Named Entity Recognition (NER)\n",
        "# ===============================\n",
        "ctp = [\"Narrator Name\", \"Cleaned Bio\", \"Interview Title\", \"Date & Location\"]\n",
        "ec = Counter()\n",
        "unique_entities = set()\n",
        "\n",
        "for column in ctp:\n",
        "    if column in fp.columns:\n",
        "        for text in fp[column].dropna():\n",
        "            recrd = nlp(text)\n",
        "            for ent in recrd.ents:\n",
        "                unique_entities.add((ent.text, ent.label_))\n",
        "\n",
        "# Count unique entities per label\n",
        "for _, label in unique_entities:\n",
        "    ec[label] += 1\n",
        "\n",
        "print(\"\\n=== Named Entity Recognition Counts ===\")\n",
        "for entity, count in ec.items():\n",
        "    print(f\"{entity}: {count}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt:Give me a code to scrape datafrom the GitHub Marketplace to get informations like product name, description, URL, and page number and format it in a csv file.Give me python code to use in Google collab."
      ],
      "metadata": {
        "id": "7vKbulH5QM0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# Session setup with larger connection pool\n",
        "\n",
        "session = requests.Session()\n",
        "adapter = HTTPAdapter(pool_connections=50, pool_maxsize=50)  # increase pool size\n",
        "session.mount('https://', adapter)\n",
        "session.mount('http://', adapter)\n",
        "session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
        "\n",
        "# --------------------------\n",
        "# Exponential backoff\n",
        "# --------------------------\n",
        "def sleep_with_backoff(attempt):\n",
        "    time.sleep(min(2 ** attempt + random.uniform(0, 1), 10))\n",
        "\n",
        "# --------------------------\n",
        "# Extract actions from a page\n",
        "# --------------------------\n",
        "def extract_actn_lnk(page_number):\n",
        "    wblk = f\"https://github.com/marketplace?type=actions&page={page_number}\"\n",
        "    actn_lnk = []\n",
        "    print(f\"Processing Page {page_number}\")\n",
        "\n",
        "    for attempt in range(5):\n",
        "        try:\n",
        "            response = session.get(wblk, timeout=10)\n",
        "            if response.status_code != 200:\n",
        "                raise Exception(f\"Status code: {response.status_code}\")\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            actions = soup.find_all(\"div\", attrs={'data-testid': 'non-featured-item'})\n",
        "\n",
        "            for action in actions:\n",
        "                ln_tg = action.find(\"a\", href=True)\n",
        "                name = ln_tg.text.strip() if ln_tg else \"N/A\"\n",
        "                link = f\"https://github.com{ln_tg['href']}\" if ln_tg else \"N/A\"\n",
        "                actn_lnk.append({\n",
        "                    \"Page Number\": page_number,\n",
        "                    \"Action Name\": name,\n",
        "                    \"Link\": link,\n",
        "                    \"Description\": \"N/A\"\n",
        "                })\n",
        "            break  # success\n",
        "        except Exception as e:\n",
        "            print(f\"Error on page {page_number}: {e}\")\n",
        "            sleep_with_backoff(attempt)\n",
        "\n",
        "    print(f\"✅ Extracted {len(actn_lnk)} actions from page {page_number}\")\n",
        "    return actn_lnk\n",
        "\n",
        "# --------------------------\n",
        "# Fetch description for an action\n",
        "# --------------------------\n",
        "def fetch_description(action):\n",
        "    wblk = action['Link']\n",
        "    for attempt in range(5):\n",
        "        try:\n",
        "            response = session.get(wblk, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'lxml')\n",
        "                div = soup.find('div', attrs={'data-testid': 'about'})\n",
        "                if div:\n",
        "                    span = div.find('span')\n",
        "                    if span:\n",
        "                        action['Description'] = span.get_text().strip()\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {wblk}: {e}\")\n",
        "            sleep_with_backoff(attempt)\n",
        "    return action\n",
        "\n",
        "# --------------------------\n",
        "# Save CSV\n",
        "# --------------------------\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['Page Number', 'Action Name', 'Link', 'Description'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "# --------------------------\n",
        "# Main scraping\n",
        "# --------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    all_actions = []\n",
        "    max_pages = 65  # adjust if needed\n",
        "\n",
        "    # Scrape all pages\n",
        "    for page_number in range(1, max_pages + 1):\n",
        "        actions = extract_actn_lnk(page_number)\n",
        "        all_actions.extend(actions)\n",
        "        if page_number % 5 == 0:\n",
        "            print(f\"Progress: {len(all_actions)} actions collected after {page_number} pages\")\n",
        "        time.sleep(random.uniform(1, 2))  # small delay between pages\n",
        "\n",
        "    print(f\"Total actions extracted before deduplication: {len(all_actions)}\")\n",
        "\n",
        "    # Remove duplicates by Link\n",
        "    unique_actions = {a['Link']: a for a in all_actions if a['Link'] != \"N/A\"}\n",
        "    unique_actions = list(unique_actions.values())\n",
        "    print(f\"Total unique actions to fetch descriptions: {len(unique_actions)}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # Concurrent description fetching\n",
        "    # --------------------------\n",
        "    print(\"Fetching descriptions concurrently...\")\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=20) as executor:\n",
        "        futre = [executor.submit(fetch_description, a) for a in unique_actions]\n",
        "        for f in tqdm(as_completed(futre), total=len(futre)):\n",
        "            results.append(f.result())\n",
        "\n",
        "    # Save final CSV\n",
        "    save_to_csv(results, \"github_actions_final_data.csv\")\n",
        "    print(f\"✅ Data extraction completed. Total actions saved: {len(results)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUmUAArzQMI9",
        "outputId": "fdd88539-a10d-4275-d8ad-7cee2a9c8bb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Page 1\n",
            "✅ Extracted 20 actions from page 1\n",
            "Processing Page 2\n",
            "✅ Extracted 20 actions from page 2\n",
            "Processing Page 3\n",
            "✅ Extracted 20 actions from page 3\n",
            "Processing Page 4\n",
            "✅ Extracted 0 actions from page 4\n",
            "Processing Page 5\n",
            "✅ Extracted 20 actions from page 5\n",
            "Progress: 80 actions collected after 5 pages\n",
            "Processing Page 6\n",
            "✅ Extracted 20 actions from page 6\n",
            "Processing Page 7\n",
            "✅ Extracted 0 actions from page 7\n",
            "Processing Page 8\n",
            "✅ Extracted 20 actions from page 8\n",
            "Processing Page 9\n",
            "✅ Extracted 20 actions from page 9\n",
            "Processing Page 10\n",
            "✅ Extracted 20 actions from page 10\n",
            "Progress: 160 actions collected after 10 pages\n",
            "Processing Page 11\n",
            "✅ Extracted 0 actions from page 11\n",
            "Processing Page 12\n",
            "✅ Extracted 20 actions from page 12\n",
            "Processing Page 13\n",
            "✅ Extracted 20 actions from page 13\n",
            "Processing Page 14\n",
            "✅ Extracted 0 actions from page 14\n",
            "Processing Page 15\n",
            "✅ Extracted 20 actions from page 15\n",
            "Progress: 220 actions collected after 15 pages\n",
            "Processing Page 16\n",
            "✅ Extracted 20 actions from page 16\n",
            "Processing Page 17\n",
            "✅ Extracted 20 actions from page 17\n",
            "Processing Page 18\n",
            "✅ Extracted 0 actions from page 18\n",
            "Processing Page 19\n",
            "✅ Extracted 20 actions from page 19\n",
            "Processing Page 20\n",
            "✅ Extracted 20 actions from page 20\n",
            "Progress: 300 actions collected after 20 pages\n",
            "Processing Page 21\n",
            "✅ Extracted 20 actions from page 21\n",
            "Processing Page 22\n",
            "✅ Extracted 20 actions from page 22\n",
            "Processing Page 23\n",
            "✅ Extracted 20 actions from page 23\n",
            "Processing Page 24\n",
            "✅ Extracted 20 actions from page 24\n",
            "Processing Page 25\n",
            "✅ Extracted 20 actions from page 25\n",
            "Progress: 400 actions collected after 25 pages\n",
            "Processing Page 26\n",
            "✅ Extracted 20 actions from page 26\n",
            "Processing Page 27\n",
            "✅ Extracted 20 actions from page 27\n",
            "Processing Page 28\n",
            "✅ Extracted 20 actions from page 28\n",
            "Processing Page 29\n",
            "✅ Extracted 20 actions from page 29\n",
            "Processing Page 30\n",
            "✅ Extracted 20 actions from page 30\n",
            "Progress: 500 actions collected after 30 pages\n",
            "Processing Page 31\n",
            "✅ Extracted 20 actions from page 31\n",
            "Processing Page 32\n",
            "✅ Extracted 20 actions from page 32\n",
            "Processing Page 33\n",
            "✅ Extracted 20 actions from page 33\n",
            "Processing Page 34\n",
            "✅ Extracted 20 actions from page 34\n",
            "Processing Page 35\n",
            "✅ Extracted 20 actions from page 35\n",
            "Progress: 600 actions collected after 35 pages\n",
            "Processing Page 36\n",
            "✅ Extracted 20 actions from page 36\n",
            "Processing Page 37\n",
            "✅ Extracted 20 actions from page 37\n",
            "Processing Page 38\n",
            "✅ Extracted 20 actions from page 38\n",
            "Processing Page 39\n",
            "✅ Extracted 20 actions from page 39\n",
            "Processing Page 40\n",
            "✅ Extracted 20 actions from page 40\n",
            "Progress: 700 actions collected after 40 pages\n",
            "Processing Page 41\n",
            "✅ Extracted 20 actions from page 41\n",
            "Processing Page 42\n",
            "✅ Extracted 20 actions from page 42\n",
            "Processing Page 43\n",
            "✅ Extracted 20 actions from page 43\n",
            "Processing Page 44\n",
            "✅ Extracted 20 actions from page 44\n",
            "Processing Page 45\n",
            "✅ Extracted 20 actions from page 45\n",
            "Progress: 800 actions collected after 45 pages\n",
            "Processing Page 46\n",
            "✅ Extracted 20 actions from page 46\n",
            "Processing Page 47\n",
            "✅ Extracted 20 actions from page 47\n",
            "Processing Page 48\n",
            "Error on page 48: Status code: 429\n",
            "Error on page 48: Status code: 429\n",
            "Error on page 48: Status code: 429\n",
            "✅ Extracted 20 actions from page 48\n",
            "Processing Page 49\n",
            "Error on page 49: Status code: 429\n",
            "✅ Extracted 20 actions from page 49\n",
            "Processing Page 50\n",
            "✅ Extracted 20 actions from page 50\n",
            "Progress: 900 actions collected after 50 pages\n",
            "Processing Page 51\n",
            "✅ Extracted 0 actions from page 51\n",
            "Processing Page 52\n",
            "✅ Extracted 20 actions from page 52\n",
            "Processing Page 53\n",
            "✅ Extracted 20 actions from page 53\n",
            "Processing Page 54\n",
            "✅ Extracted 20 actions from page 54\n",
            "Processing Page 55\n",
            "✅ Extracted 20 actions from page 55\n",
            "Progress: 980 actions collected after 55 pages\n",
            "Processing Page 56\n",
            "✅ Extracted 20 actions from page 56\n",
            "Processing Page 57\n",
            "✅ Extracted 20 actions from page 57\n",
            "Processing Page 58\n",
            "✅ Extracted 20 actions from page 58\n",
            "Processing Page 59\n",
            "✅ Extracted 20 actions from page 59\n",
            "Processing Page 60\n",
            "✅ Extracted 20 actions from page 60\n",
            "Progress: 1080 actions collected after 60 pages\n",
            "Processing Page 61\n",
            "✅ Extracted 20 actions from page 61\n",
            "Processing Page 62\n",
            "✅ Extracted 20 actions from page 62\n",
            "Processing Page 63\n",
            "✅ Extracted 20 actions from page 63\n",
            "Processing Page 64\n",
            "✅ Extracted 20 actions from page 64\n",
            "Processing Page 65\n",
            "✅ Extracted 20 actions from page 65\n",
            "Progress: 1180 actions collected after 65 pages\n",
            "Total actions extracted before deduplication: 1180\n",
            "Total unique actions to fetch descriptions: 1174\n",
            "Fetching descriptions concurrently...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1174/1174 [03:42<00:00,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data extraction completed. Total actions saved: 1174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: Give me the python code to clean the extraced file by utilising the necessary libraries for it"
      ],
      "metadata": {
        "id": "S5xubdhpTxiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# Import Libraries\n",
        "# --------------------------\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# --------------------------\n",
        "# Download NLTK resources (first time)\n",
        "# --------------------------\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")   # fixes LookupError in newer NLTK versions\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# --------------------------\n",
        "# Load Data\n",
        "# --------------------------\n",
        "my_file = \"github_actions_final_data.csv\"  # Update path if needed\n",
        "datafile = pd.read_csv(my_file)\n",
        "print(\"📊 Original dataset shape:\", datafile.shape)\n",
        "\n",
        "# --------------------------\n",
        "# Text Preprocessing Function\n",
        "# --------------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"no description available\"\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r\"<.*?>\", \" \", str(text))\n",
        "    # Keep only letters and spaces\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
        "    # Lowercase and strip\n",
        "    text = text.lower().strip()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# --------------------------\n",
        "# Data Quality Checks\n",
        "# --------------------------\n",
        "def data_quality_checks(data):\n",
        "    # Remove duplicates by unique identifier (Link)\n",
        "    before = data.shape[0]\n",
        "    data = data.drop_duplicates(subset=[\"Link\"])\n",
        "    after = data.shape[0]\n",
        "    print(f\"🔍 Removed {before - after} duplicate rows\")\n",
        "\n",
        "    # Fill missing values\n",
        "    data[\"Action Name\"] = data[\"Action Name\"].fillna(\"Unknown\")\n",
        "    data[\"Description\"] = data[\"Description\"].fillna(\"No description available\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# --------------------------\n",
        "# Apply Preprocessing (create new columns)\n",
        "# --------------------------\n",
        "datafile[\"Clean_Action_Name\"] = datafile[\"Action Name\"].apply(preprocess_text)\n",
        "datafile[\"Clean_Description\"] = datafile[\"Description\"].apply(preprocess_text)\n",
        "\n",
        "# --------------------------\n",
        "# Perform Data Quality\n",
        "# --------------------------\n",
        "final_data = data_quality_checks(datafile)\n",
        "print(\"📊 Dataset shape after cleaning:\", final_data.shape)\n",
        "\n",
        "# --------------------------\n",
        "# Save Processed Data\n",
        "# --------------------------\n",
        "output_file = \"github_actions_processed_data.csv\"\n",
        "final_data.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "print(f\"✅ Data preprocessing & quality checks completed. File saved: {output_file}\")\n",
        "\n",
        "# --------------------------\n",
        "# Mini Data Quality Report\n",
        "# --------------------------\n",
        "print(\"\\n--- Data Quality Report ---\")\n",
        "print(\"Null values per column:\\n\", final_data.isnull().sum())\n",
        "print(\"\\nAverage description length (words):\",\n",
        "      final_data[\"Clean_Description\"].apply(lambda x: len(x.split())).mean())\n",
        "print(\"Average action name length (words):\",\n",
        "      final_data[\"Clean_Action_Name\"].apply(lambda x: len(x.split())).mean())\n",
        "\n",
        "# Optional: Top 20 most common words in descriptions\n",
        "from collections import Counter\n",
        "all_words = \" \".join(final_data[\"Clean_Description\"]).split()\n",
        "most_common_words = Counter(all_words).most_common(20)\n",
        "print(\"\\nTop 20 most common words in descriptions:\")\n",
        "for word, count in most_common_words:\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK8T5G8BWpTa",
        "outputId": "0f523d3c-c467-4df3-9ac0-a15ef883d5a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Original dataset shape: (1174, 4)\n",
            "🔍 Removed 0 duplicate rows\n",
            "📊 Dataset shape after cleaning: (1174, 6)\n",
            "✅ Data preprocessing & quality checks completed. File saved: github_actions_processed_data.csv\n",
            "\n",
            "--- Data Quality Report ---\n",
            "Null values per column:\n",
            " Page Number          0\n",
            "Action Name          0\n",
            "Link                 0\n",
            "Description          0\n",
            "Clean_Action_Name    0\n",
            "Clean_Description    0\n",
            "dtype: int64\n",
            "\n",
            "Average description length (words): 6.904599659284497\n",
            "Average action name length (words): 2.82793867120954\n",
            "\n",
            "Top 20 most common words in descriptions:\n",
            "github: 401\n",
            "action: 378\n",
            "run: 136\n",
            "request: 105\n",
            "file: 101\n",
            "pull: 101\n",
            "code: 94\n",
            "build: 81\n",
            "using: 75\n",
            "workflow: 68\n",
            "release: 59\n",
            "repository: 57\n",
            "pr: 54\n",
            "check: 54\n",
            "deploy: 52\n",
            "project: 51\n",
            "automatically: 51\n",
            "version: 50\n",
            "issue: 50\n",
            "setup: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt:Give me the code to web scrape from Twitter using the Tweepy API using python. Tell me the step by step procedure to scrap specific hashtags related to subtopics like machine learning or artificial intelligence.The extracted data includes the tweet ID, username, and text."
      ],
      "metadata": {
        "id": "hfzW4I9bXnAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "api key - XRiewqo35I94XlnBkRMpsxUbR\n",
        "api-key secret-9g8NsUK8JgABAk0k92lZRaaGfDTnUEwaG2knf1WpOwSRTKzcQJ\n",
        "bearer token - AAAAAAAAAAAAAAAAAAAAAJT54QEAAAAABKCgw%2BATC%2FCgpGXYDVf6CqrUYf8%3Dd9OVqYgYOUW88x21AHVoiAttD9NSvLW9nWa5e4LgPTtXKFuMFz\n",
        "access token -1972857394087485440-7GLF55yrz883lvkxDyYZ1bN0Y8AniU\n",
        "access token secret -ChrWZwsFmLx13JomUBrCtDSGakt5o5rIU8JiGTsLu46O7\n",
        "ChrWZwsFmLx13JomUBrCtDSGakt5o5rIU8JiGTsLu46O7"
      ],
      "metadata": {
        "id": "ei58St4rhkjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UL015nLMc2IM"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Twitter API credentials\n",
        "api_key = \"XRiewqo35I94XlnBkRMpsxUbR\"\n",
        "api_key_secret = \"9g8NsUK8JgABAk0k92lZRaaGfDTnUEwaG2knf1WpOwSRTKzcQJ \"\n",
        "access_token = \"1972857394087485440-7GLF55yrz883lvkxDyYZ1bN0Y8AniU\"\n",
        "access_token_secret = \"ChrWZwsFmLx13JomUBrCtDSGakt5o5rIU8JiGTsLu46O7\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuth1UserHandler(api_key, api_key_secret, access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ],
      "metadata": {
        "id": "NrinYu8VeSG8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"(#MachineLearning OR #ArtificialIntelligence OR #AI OR #ML) -is:retweet\"\n",
        "\n",
        "# Use Twitter API v2\n",
        "client = tweepy.Client(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAJT54QEAAAAABKCgw%2BATC%2FCgpGXYDVf6CqrUYf8%3Dd9OVqYgYOUW88x21AHVoiAttD9NSvLW9nWa5e4LgPTtXKFuMFz\")\n",
        "\n",
        "tweets = client.search_recent_tweets(query=query,\n",
        "                                     tweet_fields=[\"created_at\", \"text\", \"author_id\"],\n",
        "                                     max_results=100)\n",
        "\n",
        "# Store the tweets\n",
        "tweet_data = []\n",
        "\n",
        "for tweet in tweets.data:\n",
        "    tweet_data.append({\n",
        "        'tweet_id': tweet.id,\n",
        "        'created_at': tweet.created_at,\n",
        "        'user': tweet.author_id,  # API v2 does not return screen_name directly\n",
        "        'text': tweet.text\n",
        "    })"
      ],
      "metadata": {
        "id": "2hc2CKd0iRv6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tweet_data))  # Check how many tweets were stored\n",
        "print(tweet_data[:12])   # Print first 12 tweets for verification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVsQpk4jjslP",
        "outputId": "455d4a23-6c55-42e1-c54a-e28df45bfffe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "[{'tweet_id': 1972858530324734029, 'created_at': datetime.datetime(2025, 9, 30, 2, 58, 22, tzinfo=datetime.timezone.utc), 'user': 1663161949524226055, 'text': 'Clone your voice in 11 seconds 🎤\\n…without the heavy accent 👀\\nHere’s the trick:\\n✨ Record easy sentences\\n✨ Pick Cartesia or Fish Audio\\n✨ Add emotion tags for extra flair\\n🧐Try It Today! 👉https://t.co/pn0bE6ugOF\\n\\n🎥 Watch the full tutorial 👇\\n\\n#AI #VoiceCloning #TTS https://t.co/S7uwZXhlwL'}, {'tweet_id': 1972858525622890755, 'created_at': datetime.datetime(2025, 9, 30, 2, 58, 21, tzinfo=datetime.timezone.utc), 'user': 2913601369, 'text': '🤡不是说Sonnet 4.5很牛逼吗？怎么使用感觉有点降智呢？难不成任务难度很大？一个给出demo和文档的简单任务都完成不了！！！\\n\\n#独立开发者 #buildinpublic #indiehacker #indiehackers #AI #Apple'}, {'tweet_id': 1972858509265105393, 'created_at': datetime.datetime(2025, 9, 30, 2, 58, 17, tzinfo=datetime.timezone.utc), 'user': 1918103693800685568, 'text': '🎨ブログを公開しました✨\\nテーマは『AIクリエイターの仕事とは？』👩\\u200d💻\\nAIを活用したデザインや動画編集を通して、身につくスキルをご紹介🌿\\n未経験から挑戦したい方も大歓迎です！ぜひご覧ください😊\\nhttps://t.co/QKug71Eec2\\n#就B #AIクリエイター #AI #動画編集 #デザイン #スキルアップ #初心者歓迎'}, {'tweet_id': 1972858498091561255, 'created_at': datetime.datetime(2025, 9, 30, 2, 58, 14, tzinfo=datetime.timezone.utc), 'user': 1954780698222723072, 'text': '🌘👱🎌🛐\\n💵 $128K this weekend ✅ Stock expert @MulhollowsBistr knows what he’s doing. Follow for hot picks ! \\n#AI #premarket $AIRJ $FOFO \\n #SecondaryMarket📦 $INTC  \\n #SharePrice🌉 $WFC  \\n #RiskManagement🚶 $GS  \\n #Stocks🐼$CYCN https://t.co/OVjVsMo3J6'}, {'tweet_id': 1972858495503650943, 'created_at': datetime.datetime(2025, 9, 30, 2, 58, 13, tzinfo=datetime.timezone.utc), 'user': 1870696021, 'text': 'Beware of the “Recovery” and “Advance-Fee Recovery” Scam\\n\\nRead: https://t.co/B1jxZKfQTO\\n\\n#ScamAlert #stockmarket #stocks #realestate #finance #Business #Corporate #Corporation #Entrepreneur #Philippines #Financial #PersonalFinance #AI #GoNegosyo #Invest #Investment #investingtips'}, {'tweet_id': 1972858459298410988, 'created_at': datetime.datetime(2025, 9, 30, 2, 58, 5, tzinfo=datetime.timezone.utc), 'user': 1912494320420311040, 'text': '@BanhanETH The combo of @Subzero_Labs + @RialoHQ  is next level.\\nNot just talking #AI + #Web3, but actually building the rails for the Agent Economy.\\nReal infra &gt; empty hype. Bullish'}, {'tweet_id': 1972858443993067769, 'created_at': datetime.datetime(2025, 9, 30, 2, 58, 1, tzinfo=datetime.timezone.utc), 'user': 555031989, 'text': 'Drivers and barriers to #GenerativeAI\\nby @Gartner_inc\\n\\n#ArtificialIntelligence #MachineLearning #ML #Technology \\n  \\ncc: @bernardmarr @marcusborba @JimMarous https://t.co/VK3yEk8G49'}, {'tweet_id': 1972858440528597443, 'created_at': datetime.datetime(2025, 9, 30, 2, 58, tzinfo=datetime.timezone.utc), 'user': 1103013448659566592, 'text': 'How to ensure #healthcare workers benefit from #AI via @healthcaredive\\n\\nhttps://t.co/SuZMFN3eEM'}, {'tweet_id': 1972858422044561637, 'created_at': datetime.datetime(2025, 9, 30, 2, 57, 56, tzinfo=datetime.timezone.utc), 'user': 1812099934398955520, 'text': '@kireinahosozora #自作自演\\n#ヤラセ\\n#ステマ\\n#ネット工作 …\\n\\n#修正加工\\n#合成\\n#AI \\n挙句の果てに\\n#特殊メイク\\n#美容整形 まで…\\n\\n手段を選ばない  #秋篠宮家 #お妃紀子 だけに何も信じられない  #被害者ポジション\\n\\n#天皇家を守れ\\n#直系長子継承\\n#敬宮愛子さまを皇太子に\\n\\n#秋篠宮家は日本の恥\\n#秋篠宮家不要 https://t.co/BJI18X0OOT'}, {'tweet_id': 1972858400045424643, 'created_at': datetime.datetime(2025, 9, 30, 2, 57, 51, tzinfo=datetime.timezone.utc), 'user': 1967634956286562305, 'text': 'Explore Fosshub AI, a curated product directory for AI tools. \\nFind and evaluate AI applications with ease through editorial testing and explainer videos. \\nPerfect for decision-makers seeking practical value. \\n#AI #Tools https://t.co/HysfUSnAFu'}, {'tweet_id': 1972858364162945352, 'created_at': datetime.datetime(2025, 9, 30, 2, 57, 42, tzinfo=datetime.timezone.utc), 'user': 1778681903329234944, 'text': 'Running a restaurant is tough. 💸\\n What’s tougher? Tracking every bill — rent, utilities, staff, suppliers.\\n In 2025, smart owners aren’t waiting till month-end. They use AI-powered expense tracking like ccMonet.\\n 👉 Real-time clarity, zero hassle.\\n#Restaurant #SME #AI #ccMonet https://t.co/L9DQ4bKLSB'}, {'tweet_id': 1972858323457458454, 'created_at': datetime.datetime(2025, 9, 30, 2, 57, 32, tzinfo=datetime.timezone.utc), 'user': 1866561038721880066, 'text': '@Montiweb3 @SentientAGI Privacy-first AI is the only future. 🚀 @zama_fte #AI #CryptoGiveaway'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(tweet_data)\n",
        "\n",
        "# Save as CSV file\n",
        "df.to_csv(\"tweets.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Tweets saved successfully to tweets.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PYj263zjzMe",
        "outputId": "bc759242-d52a-49bf-df72-63069079677a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweets saved successfully to tweets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_check = pd.read_csv(\"tweets.csv\")\n",
        "print(df_check.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJLo75umj5td",
        "outputId": "c6aadae1-547c-416c-fd1c-97a13af1164c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              tweet_id                 created_at                 user  \\\n",
            "0  1972858530324734029  2025-09-30 02:58:22+00:00  1663161949524226055   \n",
            "1  1972858525622890755  2025-09-30 02:58:21+00:00           2913601369   \n",
            "2  1972858509265105393  2025-09-30 02:58:17+00:00  1918103693800685568   \n",
            "3  1972858498091561255  2025-09-30 02:58:14+00:00  1954780698222723072   \n",
            "4  1972858495503650943  2025-09-30 02:58:13+00:00           1870696021   \n",
            "\n",
            "                                                text  \n",
            "0  Clone your voice in 11 seconds 🎤\\n…without the...  \n",
            "1  🤡不是说Sonnet 4.5很牛逼吗？怎么使用感觉有点降智呢？难不成任务难度很大？一个给出d...  \n",
            "2  🎨ブログを公開しました✨\\nテーマは『AIクリエイターの仕事とは？』👩‍💻\\nAIを活用した...  \n",
            "3  🌘👱🎌🛐\\n💵 $128K this weekend ✅ Stock expert @Mul...  \n",
            "4  Beware of the “Recovery” and “Advance-Fee Reco...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Install missing packages\n",
        "!pip install emoji unidecode textblob\n",
        "\n",
        "# 2️⃣ Import libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "import emoji\n",
        "import unidecode\n",
        "\n",
        "# 3️⃣ Download NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# 4️⃣ Load CSV\n",
        "df = pd.read_csv(\"/content/tweets.csv\")\n",
        "\n",
        "# 5️⃣ Data cleaning & preprocessing (as in your code)\n",
        "# Remove duplicates\n",
        "df.drop_duplicates(subset=\"tweet_id\", keep=\"first\", inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = unidecode.unidecode(text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", \"\", text)\n",
        "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = emoji.replace_emoji(text, replace=\"\")\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.lower()\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# Remove stopwords & lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "# Spell correction\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(correct_spelling)\n",
        "\n",
        "# Convert 'created_at' to datetime\n",
        "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
        "\n",
        "# Final checks\n",
        "print(\"Final dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Save cleaned CSV\n",
        "cleaned_filename = \"final_cleaned_tweets.csv\"\n",
        "df.to_csv(cleaned_filename, index=False, encoding=\"utf-8\")\n",
        "print(f\"Deep cleaning complete! Cleaned data saved as '{cleaned_filename}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeWUfQWBj_TA",
        "outputId": "87158e6e-a54a-4d49-deec-596d9a057e93"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode, emoji\n",
            "Successfully installed emoji-2.15.0 unidecode-1.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataset shape: (100, 4)\n",
            "              tweet_id                created_at                 user  \\\n",
            "0  1972858530324734029 2025-09-30 02:58:22+00:00  1663161949524226055   \n",
            "1  1972858525622890755 2025-09-30 02:58:21+00:00           2913601369   \n",
            "2  1972858509265105393 2025-09-30 02:58:17+00:00  1918103693800685568   \n",
            "3  1972858498091561255 2025-09-30 02:58:14+00:00  1954780698222723072   \n",
            "4  1972858495503650943 2025-09-30 02:58:13+00:00           1870696021   \n",
            "\n",
            "                                                text  \n",
            "0  alone voice 11 second without heavy accent her...  \n",
            "1  bu she shut bonnet when nip i zen to she long ...  \n",
            "2  buroguwogong ai shimashita temahaaikurieitanos...  \n",
            "3  128k weekend stock expert know he follow hot p...  \n",
            "4  beware recovery advancefee recovery scar read ...  \n",
            "Deep cleaning complete! Cleaned data saved as 'final_cleaned_tweets.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}